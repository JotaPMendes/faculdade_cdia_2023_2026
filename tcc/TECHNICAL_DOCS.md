# üìò Documenta√ß√£o T√©cnica

Este documento detalha o funcionamento interno dos solvers e sistemas implementados neste projeto.

## 1. Solvers Implementados

### 1.1. Finite Element Method (FEM)
O solver FEM (`models/fem.py`) serve como **baseline num√©rico** de alta precis√£o para validar os resultados da PINN e dos modelos de ML.

*   **Tipo de Elemento**: Elementos finitos Q1 (bilineares) em uma malha estruturada retangular.
*   **Formula√ß√£o Fraca**: Para a equa√ß√£o de Poisson $-\Delta u = f$, buscamos $u \in H^1_0(\Omega)$ tal que:
    $$ \int_\Omega \nabla u \cdot \nabla v \, dx = \int_\Omega f v \, dx, \quad \forall v \in H^1_0(\Omega) $$
*   **Montagem da Matriz**:
    *   A matriz de rigidez global $K$ √© montada a partir das matrizes locais $K_e$ de cada elemento $4 \times 4$.
    *   Utiliza-se `scipy.sparse.coo_matrix` para montagem eficiente e `csr_matrix` para resolu√ß√£o.
*   **Condi√ß√µes de Contorno**:
    *   Dirichlet: Os n√≥s da borda s√£o identificados e suas linhas na matriz $K$ s√£o substitu√≠das por uma identidade (m√©todo da penalidade exata/substitui√ß√£o), for√ßando o valor $u = u_{true}$.

### 1.2. Physics-Informed Neural Networks (PINN)
A PINN (`models/pinn.py`) resolve a EDP minimizando uma fun√ß√£o de perda composta pelos res√≠duos da equa√ß√£o e das condi√ß√µes de contorno.

*   **Framework**: DeepXDE com backend TensorFlow (compat.v1).
*   **Arquitetura**: `MsFFN` (Multi-scale Fourier Feature Network).
    *   Esta arquitetura √© escolhida por sua capacidade superior em aprender fun√ß√µes com altas frequ√™ncias, mitigando o "spectral bias" de redes neurais comuns.
*   **Fun√ß√£o de Perda**:
    $$ \mathcal{L} = \lambda_{PDE} \mathcal{L}_{PDE} + \lambda_{BC} \mathcal{L}_{BC} $$
    *   $\mathcal{L}_{PDE}$: M√©dia dos quadrados dos res√≠duos da equa√ß√£o diferencial nos pontos de coloca√ß√£o.
    *   $\mathcal{L}_{BC}$: Erro quadr√°tico m√©dio nos pontos de contorno.
*   **Otimiza√ß√£o H√≠brida**:
    1.  **Adam**: Otimizador estoc√°stico robusto para as primeiras 15.000 itera√ß√µes (explora√ß√£o global).
    2.  **L-BFGS**: Otimizador de segunda ordem (quasi-Newton) para refinamento final (converg√™ncia r√°pida e precisa).

#### 1.2.1. Otimiza√ß√µes Implementadas

Com base em pesquisa no reposit√≥rio oficial do DeepXDE, implementamos as seguintes otimiza√ß√µes para o problema de Poisson 2D:

**A. Ativa√ß√£o `tanh` (Revertido)**
*   **Motiva√ß√£o**: Inicialmente testamos `sin`, mas combinada com MsFFN gerou instabilidade inicial. `tanh` provou ser mais robusta para este problema espec√≠fico, mantendo a capacidade de aprendizado sem explodir gradientes.
*   **Implementa√ß√£o**: `net = dde.nn.MsFFN([2] + [64]*5 + [1], "tanh", ...)`

**B. Sobol Sampling**
*   **Motiva√ß√£o**: Sampling pseudorandom pode criar clusters e deixar regi√µes vazias. Sobol sequences (quasi-random) garantem cobertura mais uniforme do dom√≠nio, reduzindo pontos redundantes.
*   **Refer√™ncia**: DeepXDE suporta nativamente Sobol, Halton, e Hammersley sequences para melhor distribui√ß√£o espacial.
*   **Implementa√ß√£o**: `train_distribution="Sobol"` no `dde.data.PDE`
*   **Trade-off**: Warnings sobre pot√™ncias de 2 s√£o esperados, mas n√£o afetam significativamente a qualidade.

**C. Escalas Fourier Ajustadas**
*   **Motiva√ß√£o**: Testamos `[1, 5, 10, 50]`, mas a escala `50` causou explos√£o da derivada segunda ($\nabla^2 u \approx \sigma^2$), elevando a loss inicial para $10^9$.
*   **Ajuste**: Reduzimos para `[1, 5, 10]`, o que estabilizou a inicializa√ß√£o ($10^6$) mantendo boa capacidade de capturar frequ√™ncias m√©dias.
*   **Implementa√ß√£o**: `sigmas=[1, 5, 10]`

**D. Rede Mais Profunda**
*   **Motiva√ß√£o**: Aumentar de `[50]*4` para `[64]*5` (4‚Üí5 camadas, 50‚Üí64 neur√¥nios) aumenta a capacidade da rede sem overhead computacional excessivo.
*   **Refer√™ncia**: DeepXDE examples para Poisson 2D usam tipicamente 4-6 camadas com 50-100 neur√¥nios.
*   **Implementa√ß√£o**: `[2] + [64]*5 + [1]`

**E. Conjunto de Teste Separado**
*   **Motiva√ß√£o**: Sem `num_test`, DeepXDE reutiliza pontos de treino para valida√ß√£o, resultando em train_loss = test_loss (falso positivo de generaliza√ß√£o).
*   **Implementa√ß√£o**: `num_test=2000` (‚âà16% do treino) para valida√ß√£o independente.
*   **Impacto**: Permite detec√ß√£o precoce de overfitting via Early Stopping.

**F. Early Stopping**
*   **Motiva√ß√£o**: Evita desperd√≠cio de tempo em modelos que estagnaram.
*   **Implementa√ß√£o**: `dde.callbacks.EarlyStopping(min_delta=1e-4, patience=2000)`
*   **Crit√©rio**: Para se a loss n√£o melhorar `1e-4` por 2000 itera√ß√µes consecutivas.

**G. RAR (Residual-based Adaptive Refinement)**
*   **Motiva√ß√£o**: Pontos de coloca√ß√£o fixos podem n√£o capturar regi√µes de alto erro (ex: bordas ou picos da fun√ß√£o). O RAR adiciona dinamicamente pontos onde o res√≠duo da PDE √© maior.
*   **Implementa√ß√£o**: `dde.callbacks.PDEPointResampler(period=1000)`
*   **Funcionamento**: A cada 1000 itera√ß√µes, avalia o erro em um conjunto denso de candidatos e adiciona os piores pontos ao conjunto de treino.
*   **Impacto**: Melhora a precis√£o em regi√µes cr√≠ticas sem aumentar drasticamente o custo computacional total.

#### 1.2.2. Corre√ß√£o Cr√≠tica: Boundary Conditions

**Problema Identificado**: A implementa√ß√£o inicial usava `bc = dde.icbc.DirichletBC(geom, lambda X: 0.0, ...)`, for√ßando $u=0$ em **toda a borda** do `train_box` $[0, 0.6] \times [0, 1]$.

**Por que isso √© errado?**
*   A borda interna em $x=0.6$ **n√£o** faz parte da borda f√≠sica do problema original $[0,1]^2$.
*   A solu√ß√£o verdadeira em $x=0.6$ √© $u(0.6, y) = \sin(0.6\pi)\sin(\pi y) \approx 0.95\sin(\pi y) \neq 0$.
*   For√ßar $u=0$ criava um gradiente artificial massivo na interface, impedindo a PINN de aprender corretamente.

**Solu√ß√£o Implementada**:
```python
bc = dde.icbc.DirichletBC(geom, u_true, lambda X, on_b: on_b)
```
*   Agora a BC usa os **valores verdadeiros** da solu√ß√£o anal√≠tica na borda.
*   Isso garante continuidade f√≠sica e permite que a PINN aprenda a f√≠sica correta dentro do `train_box` e extrapole para fora.

**Impacto**: Esta corre√ß√£o foi **fundamental** para permitir que a PINN competisse com o FEM. Antes da corre√ß√£o, o MAE era >0.5. Depois, caiu para ~0.19.

### 1.3. Machine Learning Cl√°ssico
Os modelos de ML (`models/regressors.py`) tratam o problema como uma regress√£o pura $f(x,y) \to u$.

*   **Dados**: Treinados apenas com pontos amostrados dentro do `train_box`.
*   **Modelos**:
    *   **Random Forest**: Ensemble de √°rvores de decis√£o. Bom para capturar n√£o-linearidades, mas p√©ssimo em extrapola√ß√£o.
    *   **XGBoost**: Boosting de gradiente. Alta performance em interpola√ß√£o.
    *   **KNN**: Baseado em vizinhan√ßa. Simples, mas sofre com a maldi√ß√£o da dimensionalidade e n√£o extrapola bem.

### 1.3.1. Melhorias nos Regressores (Baselines Avan√ßados)
Para uma compara√ß√£o cient√≠fica mais justa, adicionamos dois modelos que capturam melhor a natureza cont√≠nua das EDPs:

*   **MLP (Multi-layer Perceptron)**:
    *   **Motiva√ß√£o**: Baseline direto para a PINN. Ambas s√£o redes neurais; a diferen√ßa √© que a MLP usa apenas dados (Data-driven) enquanto a PINN usa F√≠sica (Physics-informed).
    *   **Objetivo**: Isolar o ganho de performance vindo da "F√≠sica".
*   **SVR (Support Vector Regression)**:
    *   **Motiva√ß√£o**: Excelente capacidade de interpola√ß√£o para fun√ß√µes suaves em baixa dimens√£o.

## 2. Sistema de Checkpointing Avan√ßado

O sistema (`utils/checkpoint.py`) gerencia a persist√™ncia dos modelos PINN de forma inteligente.

### 2.1. Registro (`registry.json`)
Mant√©m um mapeamento entre as configura√ß√µes do experimento e os diret√≥rios de salvamento.
*   **Hash de Configura√ß√£o**: Cada conjunto de par√¢metros em `config.py` gera um hash √∫nico MD5.
*   **Run ID**: O sistema atribui um ID sequencial (`run_001`, `run_002`) para cada configura√ß√£o √∫nica.

### 2.2. Fluxo de Trabalho
1.  Ao iniciar o treino, o sistema calcula o hash da configura√ß√£o atual.
2.  Verifica no `registry.json` se essa configura√ß√£o j√° foi executada.
    *   **Sim**: Reutiliza o diret√≥rio existente e tenta retomar o treinamento (Resume).
    *   **N√£o**: Cria um novo diret√≥rio (`run_XXX`) e registra a nova configura√ß√£o.
3.  **Resume Logic**: Se um checkpoint existe, o modelo carrega os pesos e treina apenas pelas itera√ß√µes restantes (`total - current_step`).

### 2.3. Limpeza Autom√°tica
Para economizar espa√ßo, o sistema mant√©m apenas os **3 checkpoints mais recentes** em cada diret√≥rio de execu√ß√£o.

## 3. Metodologia de Compara√ß√£o e Resultados

A compara√ß√£o √© realizada em tr√™s n√≠veis:

1.  **F√≠sica (Ground Truth)**: Solu√ß√£o anal√≠tica exata.
2.  **Num√©rica (FEM)**: Aproxima√ß√£o tradicional de alta ordem. Serve para validar se a PINN est√° convergindo para a solu√ß√£o f√≠sica correta.
3.  **Dados (ML)**: Modelos "black-box" que ignoram a f√≠sica subjacente.

### M√©tricas e Resultados (Poisson 2D)
*   **MAE (Mean Absolute Error)**: Calculado no conjunto de teste (que inclui regi√µes de extrapola√ß√£o).

**Ranking Final Observado:**
1.  üèÜ **SVR**: `0.090` (Excelente interpola√ß√£o suave)
2.  ü•à **PINN**: `0.124` (Superou o baseline num√©rico FEM)
3.  ü•â **FEM**: `0.173` (Baseline num√©rico padr√£o)
4.  **MLP**: `0.211` (Rede Neural sem f√≠sica)

**Conclus√£o**: A PINN superou significativamente a MLP padr√£o (0.124 vs 0.211), provando que a incorpora√ß√£o da f√≠sica (PDE Loss) foi crucial para o aprendizado, permitindo que a rede superasse at√© mesmo o m√©todo num√©rico tradicional (FEM) em precis√£o neste cen√°rio.
